<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepFloyd Diffusion Model Exploration</title>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        /* General Styles */
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9fafc;
            color: #333;
            line-height: 1.8;
        }

        /* Header Styles */
        header {
            background: linear-gradient(to right, #6a11cb, #2575fc);
            color: white;
            text-align: center;
            padding: 30px 15px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
            letter-spacing: 1px;
        }

        /* Section Styles */
        section {
            padding: 40px 15px;
            max-width: 900px;
            margin: 0 auto;
        }
        h2 {
            color: #2575fc;
            font-size: 2rem;
            margin-bottom: 20px;
        }
        h3 {
            color: #6a11cb;
            font-size: 1.7rem;
            margin-bottom: 20px;
        }
        p {
            margin-bottom: 20px;
            font-size: 1.1rem;
        }
        .highlight {
            background: #f4f6ff;
            border-left: 4px solid #6a11cb;
            padding: 15px;
            font-size: 1rem;
            line-height: 1.6;
            margin: 20px 0;
        }

        /* Image Container */
        .image-container {
            text-align: center;
            margin: 30px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        .image-container p {
            font-size: 0.9rem;
            color: #555;
            margin-top: 10px;
        }

        /* Footer Styles */
        footer {
            text-align: center;
            padding: 20px 10px;
            background: #f4f6ff;
            border-top: 1px solid #ddd;
            font-size: 0.9rem;
            color: #555;
            margin-top: 40px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models </h1>
        <h3 style="color:white"> Written by Aaron Zheng</h3>
    </header>
    <section>
        <h2>Part A</h2>
        <h3>1.0</h3>
        <p>
            In this part, I accepted the terms and conditions of using the model from HuggingFace, specifically the 
            <strong>DeepFloyd/IF-I-XL-v1.0</strong> diffusion models, and grabbed a HuggingFace token from my account.
        </p>
        <p>
            Then I was able to pull the model using the starter code provided, pulling both Stage I and Stage II of the DeepFloyd diffusion model. 
            Afterwards, I used the precomputed text embeddings. Using the precomputed textual embeddings, I was able to get the following nice images.
        </p>
        <div class="image-container">
            <img src="image_design/init_p1.0.png" alt="Generated Images using Precomputed Textual Embeddings">
            <p><em>Generated images using precomputed textual embeddings.</em></p>
        </div>
        <p>We see that the pretrained diffusion models work!</p>
        <h3>1.1</h3>
        <p>
            In this step, I took a clean image of the Campanile which is provided as a sample image, and iteratively added noise to it based on 
            a predetermined forward pass equation. The idea is that t indicates the number of timesteps, or in this case, like the number of layers of a noising neural network.
            If t is big, then that means that the original image is passing through a greater number of layers in a noising neural network. 
            Specifically, I used the <strong>alphas_cumprod</strong> variable at all the different timesteps, denoted 
            as alpha_t, using the following equation:
        </p>
        <div class="equation">
            \[
            x_t = \sqrt{\alpha_t} \cdot x_0 + \sqrt{1-\alpha_t} \cdot \epsilon
            \]
        </div>
        <p>
            Using the equation, I was able to noise the image X_t, following a standard Gaussian distribution with mean of 
            <div class="equation">\[ \sqrt{\alpha_t} \]</div> and standard deviation of <div class="equation">\[ \sqrt{1-\alpha_t} \]</div>.
        </p>
        <div class="image-container">
            <img src="image_design/original_campanile.png" alt="Noised Image of the Campanile">
            <p><em>Original Image of the Campanile</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/noised_campanile_steps.png" alt="Noised Image of the Campanile">
            <p><em>Noised Image of the Campanile at varying timesteps (values of t)</em></p>
        </div>
        <p>Through this, the image was denoised. We can see that, based on what was given by the starter code, 
            alpha_t's value went smaller and smaller as t grew, which means that less and less of the original image x_0 contributed to the final 
            image when compared to the Gaussian noise. This will naturally lead to the image being noisier, which is the observed result.
        </p>
        <h3>1.2</h3>
        <p>Given a noisy image that we have just managed to create with layers of iterative noising operations, we now try to denoise the image</p>
        <p>For my first method, I attempt to use Gaussian Blur as a denoising strategy. Since noise is typically higher frequency, by blurring we preserve only lower frequencies, and thereby this is an 
            effective approach at removing the noise in an image.
        </p>
        <p>Obviously this has some drawbacks, most notably that this method also destroys high-frequency information from the image itself, but this is our first attempt.</p>
        <p>We try to denoise the image for each of the resulting noised images for the three different noise levels (values of t) from the previous part. Here are our results.</p>
        <div class="image-container">
            <img src="image_design/image_denoising_steps_classical_method.png" alt="Denoised Noised Image of the Campanile">
            <p><em>Denoised Images of the Campanile at varying noise levels (values of t)</em></p>
        </div>
        <p>Clearly, this is a mess and Gaussian Blur does not work so well.</p>
        <h3>1.3</h3>
        <p>Now we try to use some sort of neural network model to (intelligently) denoise. 
            We shall take advantage of something called a UNet, which is basically something that takes in something and 
            returns something else, of the same shape. The special part of this UNet is that its hidden layer is much smaller than both 
            the input and output shape, and it tends to be like a funnel, each successive hidden layer from the input reducing the hidden layer
            dimensions until the smallest dimension hidden layer is reached, then inverting the funnel and increasing the hidden layer size every time.

            We use the stage_1.unet, which is pretrained on samples of x0 and xt (i.e. original as input, noised as output). We use it to recover
            (expected) Gaussian noise for a good image by passing in the original image and getting the expected noised image, then taking their difference.

            With this noise, however, we have to do more processing, as we have to then multiply this noise by <div class="equation">\[ \sqrt{1-\alpha_t} \]</div>
            <p>, then subtract the clean image from the noised image, as this method respects our original forward pass.
            </p> 
            
            <p>Running the forward passes on a set of images, we have the following result:
            </p><div class="image-container">
                <img src="image_design/model_based_denoising.png" alt="Denoised Noised Image of the Campanile">
                <p><em>Denoised Images of the Campanile at varying noise levels (values of t) with One Step Denoising, compared to Original and the Noised Image</em></p>
            </div>
            
        </p>
        <p>With this, we conclude the One-Step solution is clearly more effective than the Gaussian Denoising solution.</p>
        <h3>1.4</h3>
        <p>Can we do better, even though the last part could achieve good results?</p>
        <p>Here, we take advantage of diffusion model's nature, which is that they are supposed to be used to denoise in an iterative process</p>
        <p>Since our forward pass took T=1000 steps, we should (in theory) also take this many steps in denoising. But we can skip steps for the sake of minimzation of inference latency.</p>
        <p>We instead use strided timesteps, and our iteratively denoised equation should be written as follows</p>
        <div class="image-container">
            <img src="image_design/eqn_iterative.png" alt="Iterative Denoising Equation">
            <p><em>Cited Berkeley</em></p>
        </div>
        <p>To test this model, I noise the test Campanile image to nearly the max, at t=990, and attempt to iteratively denoise. I use a stride of 30, and output my result every 5 strides. Here are my results.</p>
        <div class="image-container">
            <img src="image_design/strided_outputs_iterative_noising.png" alt="Iterative Denoising Equation">
            <p><em>Strided Image, Noised Results</em></p>
        </div>
        <p>Clearly the rightmost image has been the most noisy, and has gotten much better as the solution iteratively denoises the noisy image.</p>
        <p>The final denoised result with iterative denoising of a t=990 noised image of the campanile is as follows:</p>
        <div class="image-container">
            <img src="image_design/iterative_denoising_result.png" alt="Iterative Denoising Equation">
            <p><em>Result of Iterative Denoising</em></p>
        </div>
        <p>Here is result of One Step Denoising of the same image:</p>
        <div class="image-container">
            <img src="image_design/one_step_denoising_of_very_noised.png" alt="Iterative Denoising Equation">
            <p><em>Result of One Step Denoising</em></p>
        </div>
        <p>Here is result of Gaussian Denoising of the same image:</p>
        <div class="image-container">
            <img src="image_design/gaussian_denoising.png" alt="Iterative Denoising Equation">
            <p><em>Result of Gaussian Denoising</em></p>
        </div>
        <p>Now seen side by side, all of them put together</p>
        <div class="image-container">
            <img src="image_design/all_denoise_1.4_together.png" alt="Iterative Denoising Equation">
            <p><em>Comparison of All Denoising Types and Categories</em></p>
        </div>
        <h3>1.5</h3>
        <p>Since we can denoise very very noised images, what if we pass in images of pure random noise to the iterative denoiser?</p>
        <p>We can, and using the same text embeddings for the unet as for the last question (the text embedding for "a high quality image"), we get the following results:</p>
        <div class="image-container">
            <img src="image_design/generated_images_1.5.png" alt="Iterative Denoising Equation">
            <p><em>Newly Generated Images</em></p>
        </div>
        <p>We see all these look somewhat real, but they can clearly look better.</p>
        <h3>1.6</h3>
        <p>To get better results, we essentially try to make it look more "real". The way we do this is kind of like inspiration from Project 3, where we create the extrapolation from the mean.
            The idea is basically that we want to have a conditional and unconditional noise estimate (noise estimate of conditional prompt and noise estimate of 
            unconditional prompt, i.e. noise estimate of u-net when you pass into it a text embedding representing a null string). Then, we extrapolate the image towards the conditional estimate by factor of gamma.
        </p>
        <div class="equation">
            \[
            \epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)
            \]
        </div>
        <p>With this extrapolation, we can get images that lean closer to the side of the image that we want to generate, based on our text embedding, than previously.</p>
        <p>Here are our results (gamma=7):</p>
        <div class="image-container">
            <img src="image_design/better_extrapolated_generation.png" alt="Iterative Denoising Equation">
            <p><em>Extrapolated (Better) Generated Images</em></p>
        </div>
        <p>The results are much better!</p>
        <h3>1.7</h3>
        <p>Now lets play more with this iterative denoising model. We want it to generate cooler things!</p>
        <p>Suppose we noise an image and we have the model regenerate a denoised image. Since this model is doing the extrapolation thing with gamma=7, we can expect very creative 
            results, and very good looking results. But do these good looking results (i.e creative results) match what we initially put in, the real result?
        </p>
        <p>Showing a subset of the results I generated, we have:</p>
        <p>1: Original Test</p>
        <div class="image-container">
            <img src="image_design/campanile_original.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/campanile_edited_17.png" alt="Iterative Denoising Equation">
            <p><em>Campanile Edited Image</em></p>
        </div>
        <p>2: Berkeley Tree</p>
        <div class="image-container">
            <img src="image_design/berkeley_tree.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/berkeley_Tree_edited.png" alt="Iterative Denoising Equation">
            <p><em>Berkeley Tree Edited Image</em></p>
        </div>
        <p>3: Cottage Monterey</p>
        <div class="image-container">
            <img src="image_design/cottage_monterey.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/cottage_monterey_edited.png" alt="Iterative Denoising Equation">
            <p><em>Cottage Monterey Edited Image</em></p>
        </div>
        <p>Wow! The denoising completely changed the image!</p>
        <h3>1.7.1</h3>
        <p>Now we do the same thing for handdrawn / clipart / cartoon images.</p>
        <p>1: Random Avocado</p>
        <div class="image-container">
            <img src="image_design/apricot.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/denoising_steps_apricot.png" alt="Iterative Denoising Equation">
            <p><em>Avocado Edited Image</em></p>
        </div>
        <p>2: Cartoon Car</p>
        <div class="image-container">
            <img src="image_design/cartoon_car.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/edited_cartoon_car.png" alt="Iterative Denoising Equation">
            <p><em>Cartoon Car Edited Image</em></p>
        </div>
        <p>3: Handdrawn Image 1</p>
        <div class="image-container">
            <img src="image_design/handdrawn1.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn1_edited.png" alt="Iterative Denoising Equation">
            <p><em>First Handdrawn Edited Image</em></p>
        </div>
        <p>4: Handdrawn Image 2</p>
        <div class="image-container">
            <img src="image_design/handdrawn2.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn2_edited.png" alt="Iterative Denoising Equation">
            <p><em>Second Handdrawn Edited Image</em></p>
        </div>
        <p>5: Handdrawn Image 3</p>
        <div class="image-container">
            <img src="image_design/handdrawn3.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn3_edited.png" alt="Iterative Denoising Equation">
            <p><em>Third Handdrawn Edited Image</em></p>
        </div>
        <h3>1.7.2</h3>
        <p>Could we now try to only change a segment of the image, i.e., have the diffusion model only selectively edit the image?
            Perhaps only the head of a person in an image, or only the tip of a tower? Lets try.
        </p>
        <p>We make a mask of pixels (1 somewhere, 0 elsewhere) and have the mask be the same shape as the image. Then, using 
            an equation, we force x_t to have the same values as the original, at pixels where the mask equals 0. 
        </p>
        <div class="equation">
            \[
            x_t = mx_t + (1 - m)f(x_{\text{orig}}, t)
            \]
        </div>        
        <p>where f is the forward pass.</p>
        <p>Our results are as follows:</p>
        <p>Original Campanile</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace1.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/campanile_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Original Image</em></p>
        </div>
        <p>Hat Person</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace2.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/person_hat_weird_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Person Image</em></p>
        </div>
        <p>UCSD Library</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace3.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/UCSD_Library_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Person Image</em></p>
        </div>
        <h3>1.7.3</h3>
        <p>Now we shall attempt to guide the image generation process to something interesting, rather than just "a high quality photo".</p>
        <p>Prompt: "a rocket ship" with original test image</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_campanile.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Campanile</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Stanford Hoover Tower</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hoover.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Hoover</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Random Tree</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_tree.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Tree</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Hospital</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hospital.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Hospital</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Church</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_church.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Church</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Canton Tower</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_canton.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Canton Tower</em></p>
        </div>
        <p>Prompt: "a rocket ship" with HK skyline</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hk.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship HK</em></p>
        </div>
        <h3>1.8</h3>
        <p>Here, we will be creating visual 
            anagrams, images that look one way when looked
             from a certain angle (for example, right side up), 
             and a different way when flipped.</p>
        <p>In order to do this, we must obtain two noise results based on the two desired outcomes, and average the noises 
            to get the final desired noise.
        </p>
        <p>Our noise algorithm is as follows:</p>
        <div class="image-container">
            <img src="image_design/anagram_algo.png" alt="Iterative Denoising Equation">
            <p><em>Anagram Algorithm</em></p>
        </div>
        <p>Where flip is flipping the image 180 degrees across an axis (here the horizontal one).</p>
        <p>We create anagrams for the case of "A visual anagram where on one orientation "an oil painting of people around a campfire" is displayed and, when flipped, "an oil painting of an old man" is displayed." Here is the results:</p>
        <div class="image-container">
            <img src="image_design/anagrams_old_man_people_campfire.png" alt="Iterative Denoising Equation">
            <p><em>People around campfire + Old Man</em></p>
        </div>
        <p>Now for "a pencil" when normally seen and "a rocket ship" when flipped</p>
        <div class="image-container">
            <img src="image_design/anagram_pencil_rocket.png" alt="Iterative Denoising Equation">
            <p><em>Pencil + Rocket</em></p>
        </div>
        <p>Now, "a photo of a hipster barista" when normally seen and "a photo of a dog" when flipped</p>
        <div class="image-container">
            <img src="image_design/anagram_hipster_dog.png" alt="Iterative Denoising Equation">
            <p><em>Hipster Barista + Dog</em></p>
        </div>
        <h3>1.9</h3>
        <p>Now we will do the final step of this project, which is to create hybrid images, images that look one way when seen from close, and another way when seen from afar.</p>
        <p>In order to do this, we must take the two noises that are generated from the differing text embedding prompts, and pass one of them through a high pass filter and the other through 
            a low pass filter. Then, we set the final noise to the sum of these two noises. 
        </p>
        <p>Here are our results:</p>
        <p>"Lithography of waterfalls" when far away and "Lithography of skull" when close</p>
        <div class="image-container">
            <img src="image_design/waterfall_skull.png" alt="Iterative Denoising Equation">
            <p><em>Waterfall + Skull</em></p>
        </div>
        <p>"People around campfire" when far away and "Snowy Mountain Village" when close</p>
        <div class="image-container">
            <img src="image_design/snowy_mountain_village_people_campfire.png" alt="Iterative Denoising Equation">
            <p><em>People around campfire + Snowy mountain village</em></p>
        </div>
        <p>"Man wearing a hat" when far away and "Rocket ship" when close</p>
        <div class="image-container">
            <img src="image_design/hatman_rocket_ship.png" alt="Iterative Denoising Equation">
            <p><em>Hat Man + Rocket ship</em></p>
        </div>
    </section>
</body>
</html>
