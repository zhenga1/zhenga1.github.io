<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepFloyd Diffusion Model Exploration</title>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        /* General Styles */
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9fafc;
            color: #333;
            line-height: 1.8;
        }

        /* Header Styles */
        header {
            background: linear-gradient(to right, #6a11cb, #2575fc);
            color: white;
            text-align: center;
            padding: 30px 15px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
            letter-spacing: 1px;
        }

        /* Section Styles */
        section {
            padding: 40px 15px;
            max-width: 900px;
            margin: 0 auto;
        }
        h2 {
            color: #2575fc;
            font-size: 2rem;
            margin-bottom: 20px;
        }
        h3 {
            color: #6a11cb;
            font-size: 1.7rem;
            margin-bottom: 20px;
        }
        p {
            margin-bottom: 20px;
            font-size: 1.1rem;
        }
        .highlight {
            background: #f4f6ff;
            border-left: 4px solid #6a11cb;
            padding: 15px;
            font-size: 1rem;
            line-height: 1.6;
            margin: 20px 0;
        }

        /* Image Container */
        .image-container {
            text-align: center;
            margin: 30px 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        .image-container p {
            font-size: 0.9rem;
            color: #555;
            margin-top: 10px;
        }

        /* Footer Styles */
        footer {
            text-align: center;
            padding: 20px 10px;
            background: #f4f6ff;
            border-top: 1px solid #ddd;
            font-size: 0.9rem;
            color: #555;
            margin-top: 40px;
        }
        /* Sidebar styles */
        #sidebar {
            width: 15%; 
            height: calc(100vh - 0px);
            overflow-y: auto;
            background-color: #f4f4f4;
            position: sticky;
            padding: 10px;
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.1);
            position: fixed;
        }
        #sidebar h2 {
            font-size: 20px;
            color: #333;
            margin-bottom: 10px;
            text-align: center;
        }

        #toc a {
            display: block;
            margin: 10px 0;
            padding: 8px 12px;
            background-color: #ffffff;
            color: #007bff;
            text-decoration: none;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            font-size: 14px;
        }

        #toc a.h3 {
            margin-left: 20px;
            background-color: #f1f1f1;
        }

        #toc a:hover {
            background-color: #e9ecef;
            color: #0056b3;
        }
        #content {
            margin-left: 20%;
            padding: 20px;
            flex: 1;
        }
    </style>
    
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models </h1>
        <h3 style="color:white"> Written by Aaron Zheng</h3>
    </header>
    <div id="sidebar">
        <h2>Table of Contents</h2>
        <div id="toc"></div>
    </div>
    <section id="content">
        <h2>Part A</h2>
        <h3>1.0</h3>
        <p>
            In this part, I accepted the terms and conditions of using the model from HuggingFace, specifically the 
            <strong>DeepFloyd/IF-I-XL-v1.0</strong> diffusion models, and grabbed a HuggingFace token from my account.
        </p>
        <p>
            Then I was able to pull the model using the starter code provided, pulling both Stage I and Stage II of the DeepFloyd diffusion model. 
            Afterwards, I used the precomputed text embeddings. Using the precomputed textual embeddings, I was able to get the following nice images.
        </p>
        <div class="image-container">
            <img src="image_design/init_p1.0.png" alt="Generated Images using Precomputed Textual Embeddings">
            <p><em>Generated images using precomputed textual embeddings.</em></p>
        </div>
        <p>We see that the pretrained diffusion models work!</p>
        <h3>1.1</h3>
        <p>
            In this step, I took a clean image of the Campanile which is provided as a sample image, and iteratively added noise to it based on 
            a predetermined forward pass equation. The idea is that t indicates the number of timesteps, or in this case, like the number of layers of a noising neural network.
            If t is big, then that means that the original image is passing through a greater number of layers in a noising neural network. 
            Specifically, I used the <strong>alphas_cumprod</strong> variable at all the different timesteps, denoted 
            as alpha_t, using the following equation:
        </p>
        <div class="equation">
            \[
            x_t = \sqrt{\alpha_t} \cdot x_0 + \sqrt{1-\alpha_t} \cdot \epsilon
            \]
        </div>
        <p>
            Using the equation, I was able to noise the image X_t, following a standard Gaussian distribution with mean of 
            <div class="equation">\[ \sqrt{\alpha_t} \]</div> and standard deviation of <div class="equation">\[ \sqrt{1-\alpha_t} \]</div>.
        </p>
        <div class="image-container">
            <img src="image_design/original_campanile.png" alt="Noised Image of the Campanile">
            <p><em>Original Image of the Campanile</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/noised_campanile_steps.png" alt="Noised Image of the Campanile">
            <p><em>Noised Image of the Campanile at varying timesteps (values of t)</em></p>
        </div>
        <p>Through this, the image was denoised. We can see that, based on what was given by the starter code, 
            alpha_t's value went smaller and smaller as t grew, which means that less and less of the original image x_0 contributed to the final 
            image when compared to the Gaussian noise. This will naturally lead to the image being noisier, which is the observed result.
        </p>
        <h3>1.2</h3>
        <p>Given a noisy image that we have just managed to create with layers of iterative noising operations, we now try to denoise the image</p>
        <p>For my first method, I attempt to use Gaussian Blur as a denoising strategy. Since noise is typically higher frequency, by blurring we preserve only lower frequencies, and thereby this is an 
            effective approach at removing the noise in an image.
        </p>
        <p>Obviously this has some drawbacks, most notably that this method also destroys high-frequency information from the image itself, but this is our first attempt.</p>
        <p>We try to denoise the image for each of the resulting noised images for the three different noise levels (values of t) from the previous part. Here are our results.</p>
        <div class="image-container">
            <img src="image_design/image_denoising_steps_classical_method.png" alt="Denoised Noised Image of the Campanile">
            <p><em>Denoised Images of the Campanile at varying noise levels (values of t)</em></p>
        </div>
        <p>Clearly, this is a mess and Gaussian Blur does not work so well.</p>
        <h3>1.3</h3>
        <p>Now we try to use some sort of neural network model to (intelligently) denoise. 
            We shall take advantage of something called a UNet, which is basically something that takes in something and 
            returns something else, of the same shape. The special part of this UNet is that its hidden layer is much smaller than both 
            the input and output shape, and it tends to be like a funnel, each successive hidden layer from the input reducing the hidden layer
            dimensions until the smallest dimension hidden layer is reached, then inverting the funnel and increasing the hidden layer size every time.

            We use the stage_1.unet, which is pretrained on samples of x0 and xt (i.e. original as input, noised as output). We use it to recover
            (expected) Gaussian noise for a good image by passing in the original image and getting the expected noised image, then taking their difference.

            With this noise, however, we have to do more processing, as we have to then multiply this noise by <div class="equation">\[ \sqrt{1-\alpha_t} \]</div>
            <p>, then subtract the clean image from the noised image, as this method respects our original forward pass.
            </p> 
            
            <p>Running the forward passes on a set of images, we have the following result:
            </p><div class="image-container">
                <img src="image_design/model_based_denoising.png" alt="Denoised Noised Image of the Campanile">
                <p><em>Denoised Images of the Campanile at varying noise levels (values of t) with One Step Denoising, compared to Original and the Noised Image</em></p>
            </div>
            
        </p>
        <p>With this, we conclude the One-Step solution is clearly more effective than the Gaussian Denoising solution.</p>
        <h3>1.4</h3>
        <p>Can we do better, even though the last part could achieve good results?</p>
        <p>Here, we take advantage of diffusion model's nature, which is that they are supposed to be used to denoise in an iterative process</p>
        <p>Since our forward pass took T=1000 steps, we should (in theory) also take this many steps in denoising. But we can skip steps for the sake of minimzation of inference latency.</p>
        <p>We instead use strided timesteps, and our iteratively denoised equation should be written as follows</p>
        <div class="image-container">
            <img src="image_design/eqn_iterative.png" alt="Iterative Denoising Equation">
            <p><em>Cited Berkeley</em></p>
        </div>
        <p>To test this model, I noise the test Campanile image to nearly the max, at t=990, and attempt to iteratively denoise. I use a stride of 30, and output my result every 5 strides. Here are my results.</p>
        <div class="image-container">
            <img src="image_design/strided_outputs_iterative_noising.png" alt="Iterative Denoising Equation">
            <p><em>Strided Image, Noised Results</em></p>
        </div>
        <p>Clearly the rightmost image has been the most noisy, and has gotten much better as the solution iteratively denoises the noisy image.</p>
        <p>The final denoised result with iterative denoising of a t=990 noised image of the campanile is as follows:</p>
        <div class="image-container">
            <img src="image_design/iterative_denoising_result.png" alt="Iterative Denoising Equation">
            <p><em>Result of Iterative Denoising</em></p>
        </div>
        <p>Here is result of One Step Denoising of the same image:</p>
        <div class="image-container">
            <img src="image_design/one_step_denoising_of_very_noised.png" alt="Iterative Denoising Equation">
            <p><em>Result of One Step Denoising</em></p>
        </div>
        <p>Here is result of Gaussian Denoising of the same image:</p>
        <div class="image-container">
            <img src="image_design/gaussian_denoising.png" alt="Iterative Denoising Equation">
            <p><em>Result of Gaussian Denoising</em></p>
        </div>
        <p>Now seen side by side, all of them put together</p>
        <div class="image-container">
            <img src="image_design/all_denoise_1.4_together.png" alt="Iterative Denoising Equation">
            <p><em>Comparison of All Denoising Types and Categories</em></p>
        </div>
        <h3>1.5</h3>
        <p>Since we can denoise very very noised images, what if we pass in images of pure random noise to the iterative denoiser?</p>
        <p>We can, and using the same text embeddings for the unet as for the last question (the text embedding for "a high quality image"), we get the following results:</p>
        <div class="image-container">
            <img src="image_design/generated_images_1.5.png" alt="Iterative Denoising Equation">
            <p><em>Newly Generated Images</em></p>
        </div>
        <p>We see all these look somewhat real, but they can clearly look better.</p>
        <h3>1.6</h3>
        <p>To get better results, we essentially try to make it look more "real". The way we do this is kind of like inspiration from Project 3, where we create the extrapolation from the mean.
            The idea is basically that we want to have a conditional and unconditional noise estimate (noise estimate of conditional prompt and noise estimate of 
            unconditional prompt, i.e. noise estimate of u-net when you pass into it a text embedding representing a null string). Then, we extrapolate the image towards the conditional estimate by factor of gamma.
        </p>
        <div class="equation">
            \[
            \epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)
            \]
        </div>
        <p>With this extrapolation, we can get images that lean closer to the side of the image that we want to generate, based on our text embedding, than previously.</p>
        <p>Here are our results (gamma=7):</p>
        <div class="image-container">
            <img src="image_design/better_extrapolated_generation.png" alt="Iterative Denoising Equation">
            <p><em>Extrapolated (Better) Generated Images</em></p>
        </div>
        <p>The results are much better!</p>
        <h3>1.7</h3>
        <p>Now lets play more with this iterative denoising model. We want it to generate cooler things!</p>
        <p>Suppose we noise an image and we have the model regenerate a denoised image. Since this model is doing the extrapolation thing with gamma=7, we can expect very creative 
            results, and very good looking results. But do these good looking results (i.e creative results) match what we initially put in, the real result?
        </p>
        <p>We vary the noise level to manipulate our results. The noise level indicates the distance from pure noise the starting image is. 1 indicates a greater amount for noise that we could add to the image, as it indicates 
            an image that is much closer to pure noise than an image represented by a larger noise level. 
        </p>
        <p>Showing a subset of the results I generated, we have:</p>
        <p>1: Original Test</p>
        <div class="image-container">
            <img src="image_design/campanile_original.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/campanile_edited_17.png" alt="Iterative Denoising Equation">
            <p><em>Campanile Edited Image</em></p>
        </div>
        <p>2: Berkeley Tree</p>
        <div class="image-container">
            <img src="image_design/berkeley_tree.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/berkeley_Tree_edited.png" alt="Iterative Denoising Equation">
            <p><em>Berkeley Tree Edited Image</em></p>
        </div>
        <p>3: Cottage Monterey</p>
        <div class="image-container">
            <img src="image_design/cottage_monterey.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/cottage_monterey_edited.png" alt="Iterative Denoising Equation">
            <p><em>Cottage Monterey Edited Image</em></p>
        </div>
        <p>Wow! The denoising completely changed the image!</p>
        <h3>1.7.1</h3>
        <p>Now we do the same thing for handdrawn / clipart / cartoon images.</p>
        <p>1: Random Avocado</p>
        <div class="image-container">
            <img src="image_design/apricot.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/denoising_steps_apricot.png" alt="Iterative Denoising Equation">
            <p><em>Avocado Edited Image</em></p>
        </div>
        <p>2: Cartoon Car</p>
        <div class="image-container">
            <img src="image_design/cartoon_car.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/edited_cartoon_car.png" alt="Iterative Denoising Equation">
            <p><em>Cartoon Car Edited Image</em></p>
        </div>
        <p>3: Handdrawn Image 1</p>
        <div class="image-container">
            <img src="image_design/handdrawn1.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn1_edited.png" alt="Iterative Denoising Equation">
            <p><em>First Handdrawn Edited Image</em></p>
        </div>
        <p>4: Handdrawn Image 2</p>
        <div class="image-container">
            <img src="image_design/handdrawn2.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn2_edited.png" alt="Iterative Denoising Equation">
            <p><em>Second Handdrawn Edited Image</em></p>
        </div>
        <p>5: Handdrawn Image 3</p>
        <div class="image-container">
            <img src="image_design/handdrawn3.png" alt="Iterative Denoising Equation">
            <p><em>Original</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/handdrawn3_edited.png" alt="Iterative Denoising Equation">
            <p><em>Third Handdrawn Edited Image</em></p>
        </div>
        <h3>1.7.2</h3>
        <p>Could we now try to only change a segment of the image, i.e., have the diffusion model only selectively edit the image?
            Perhaps only the head of a person in an image, or only the tip of a tower? Lets try.
        </p>
        <p>We make a mask of pixels (1 somewhere, 0 elsewhere) and have the mask be the same shape as the image. Then, using 
            an equation, we force x_t to have the same values as the original, at pixels where the mask equals 0. 
        </p>
        <div class="equation">
            \[
            x_t = mx_t + (1 - m)f(x_{\text{orig}}, t)
            \]
        </div>        
        <p>where f is the forward pass.</p>
        <p>Our results are as follows:</p>
        <p>Original Campanile</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace1.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/campanile_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Original Image</em></p>
        </div>
        <p>Hat Person</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace2.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/person_hat_weird_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Person Image</em></p>
        </div>
        <p>UCSD Library</p>
        <div class="image-container">
            <img src="image_design/original_mask_replace3.png" alt="Iterative Denoising Equation">
            <p><em>Original, Mask, Replaced Section</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/UCSD_Library_impaint.png" alt="Iterative Denoising Equation">
            <p><em>Impainted Person Image</em></p>
        </div>
        <h3>1.7.3</h3>
        <p>Now we shall attempt to guide the image generation process to something interesting, rather than just "a high quality photo".</p>
        <p>Prompt: "a rocket ship" with original test image</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_campanile.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Campanile</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Stanford Hoover Tower</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hoover.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Hoover</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Random Tree</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_tree.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Tree</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Hospital</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hospital.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Hospital</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Church</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_church.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Church</em></p>
        </div>
        <p>Prompt: "a rocket ship" with Canton Tower</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_canton.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship Canton Tower</em></p>
        </div>
        <p>Prompt: "a rocket ship" with HK skyline</p>
        <div class="image-container">
            <img src="image_design/rocket_ship_hk.png" alt="Iterative Denoising Equation">
            <p><em>Rocket Ship HK</em></p>
        </div>
        <h3>1.8</h3>
        <p>Here, we will be creating visual 
            anagrams, images that look one way when looked
             from a certain angle (for example, right side up), 
             and a different way when flipped.</p>
        <p>In order to do this, we must obtain two noise results based on the two desired outcomes, and average the noises 
            to get the final desired noise.
        </p>
        <p>Our noise algorithm is as follows:</p>
        <div class="image-container">
            <img src="image_design/anagram_algo.png" alt="Iterative Denoising Equation">
            <p><em>Anagram Algorithm</em></p>
        </div>
        <p>Where flip is flipping the image 180 degrees across an axis (here the horizontal one).</p>
        <p>We create anagrams for the case of "A visual anagram where on one orientation "an oil painting of people around a campfire" is displayed and, when flipped, "an oil painting of an old man" is displayed." Here is the results:</p>
        <div class="image-container">
            <img src="image_design/anagrams_old_man_people_campfire.png" alt="Iterative Denoising Equation">
            <p><em>People around campfire + Old Man</em></p>
        </div>
        <p>Now for "a pencil" when normally seen and "a rocket ship" when flipped</p>
        <div class="image-container">
            <img src="image_design/anagram_pencil_rocket.png" alt="Iterative Denoising Equation">
            <p><em>Pencil + Rocket</em></p>
        </div>
        <p>Now, "a photo of a hipster barista" when normally seen and "a photo of a dog" when flipped</p>
        <div class="image-container">
            <img src="image_design/anagram_hipster_dog.png" alt="Iterative Denoising Equation">
            <p><em>Hipster Barista + Dog</em></p>
        </div>
        <h3>1.9</h3>
        <p>Now we will do the final step of this project, which is to create hybrid images, images that look one way when seen from close, and another way when seen from afar.</p>
        <p>In order to do this, we must take the two noises that are generated from the differing text embedding prompts, and pass one of them through a high pass filter and the other through 
            a low pass filter. Then, we set the final noise to the sum of these two noises. 
        </p>
        <p>Here are our results:</p>
        <p>"Lithography of waterfalls" when far away and "Lithography of skull" when close</p>
        <div class="image-container">
            <img src="image_design/skull_waterfall_new.png" alt="Iterative Denoising Equation">
            <p><em>Waterfall + Skull</em></p>
        </div>
        <p>"People around campfire" when far away and "Snowy Mountain Village" when close</p>
        <div class="image-container">
            <img src="image_design/snowy_mountain_village_people_campfire_neww.png" alt="Iterative Denoising Equation">
            <p><em>People around campfire + Snowy mountain village</em></p>
        </div>
        <p>"Rocket ship" when far away and "Man wearing a hat" when close</p>
        <div class="image-container">
            <img src="image_design/man_rocketship.png" alt="Iterative Denoising Equation">
            <p><em>Hat Man + Rocket ship</em></p>
        </div>
        <p>We can see the last result is a little bit weird. I believe this is because the task is very challenging to generate, and the (pretrained) model is perhaps not powerful enough
            to generate good results for Hat Man and Rocket Ship.
        </p>
        <h2>Part B</h2>
        <p>Such cool results from Part A! However, in Part A, we have only been using other people's diffusion models? How do they work, and can we build our own diffusion model! In this section I will be building my own diffusion models, 
            which will serve the noble purpose of denoising images, as well as generating them! What fun!
        </p>
        <h3>1</h3>
        <h3>1.1</h3>
        <p>To start off the second part of the project, we have to build a UNet, and this Unet should be shaped exactly how the one we used from Part A is shaped. It should have
            same dimensions for input and output, and a hidden dimension within the UNet which contains a more compact representation of the information that is passed in the input. 
            The reason for this compactness is that we hope the denoiser can get a sort of compact representation of core properties of the noised image from the information represented in the initial image. So, the part of the model 
            from the input to this compact representation can be thought of as a projection of the (noisy) input image down to a compact representation, and the latter part (compact -> output), can be thought of as reconstruction of an 
            image from the compact representation.
        </p>
        <p>The structure of the UNet Model is as follows:</p>
        <div class="image-container">
            <img src="image_design/unconditional_arch.png" alt="Unconditional Architecture">
            <p><em>Architecture for basic UNet</em></p>
        </div>
        <p>Here is what each atomic model substructure represents:</p>
        <div class="image-container">
            <img src="image_design/atomic_ops_new.png" alt="Atomic Architecture">
            <p><em>Atomic Architecture</em></p>
        </div>
        <p>Here, <b>D</b> consists of the hidden dimension, and the more compact representation is derived from the fact that the image is shrunk to a latent representation of 1x1 from 28x28. The size of D kind of determines
        the number of nodes of information that we want to keep track of.</p>
        <h3>1.2</h3>
        <p>Here, our goal is to successfully implement a noising scheme for the clean image, which we import from <b>torchvision.datasets.MNIST</b>. Using our noising scheme, we then will have
             data to train our denoiser. This data naturally will be (input, output) pairs of noised and clean versions of the same image</p>
        <p>With this data, we define a hyperparameter sigma which is the fraction of noise we include in the image, acting as a coefficient of the standard gaussian noise.
            We vary sigma (coefficient of Gaussian noise) from an array ranging from 0-1.
        </p>
        <p> Noised Images with respect to Sigma looks like this:</p>
        <div class="image-container">
            <img src="image_design/noised_images_over_sigma.png" alt="Atomic Architecture">
            <p><em>Noised Images over Different Sigma</em></p>
        </div>
        <p>The loss we will optimize over is:</p>
        <div class="equation">
            \[
            L = \mathbb{E}_{z, x} || D_{\theta} (z) -x||^2
            \]
        </div>
        <p>where D_theta is the denoising model, and z is the noised image we will pass in as input.</p>
        <h3>1.2.1</h3>
        <p>Now we train the UNet model!</p>
        <p>We train using the architecture described above! Here, we exclusively use images noised with sigma = 0.5. We use a batch size of 256, and an Adam optimizer system of 1e-4. Our hidden dimension of the UNet, D, we set it to be 128, 
            allowing our diffusion model to capture ample features from the input image. 
        </p>
        <p>With the course staff's recommendation, we train the diffusion model over 5 epochs and plot the average loss over each step (i.e. each batch). Here is our Log Loss varied over the steps.</p>
        <div class="image-container">
            <img src="image_design/log_loss_base_unet.png" alt="Log Loss Base">
            <p><em>Log Loss over Steps for Unconditional UNet</em></p>
        </div>
        <p>Below is the results of training over 1 epoch!</p>
        <div class="image-container">
            <img src="image_design/results_1_epochs.png" alt="Results 1 epoch">
            <p><em>(Original, Noised, Denoised) images after 1 epoch of UNet Training</em></p>
        </div>
        <p>Below is the results of training over 5 epochs!</p>
        <div class="image-container">
            <img src="image_design/results_5_epochs.png" alt="Results 5 epoch">
            <p><em>(Original, Noised, Denoised) images after 5 epoch of UNet Training</em></p>
        </div>
        <p>We see that the model at 5 epochs of training is clearly a bit better at denoising the test image, but the 1 epoch model performs very impressively already. </p>
        <h3>1.2.2</h3>
        <p>What if we passed in inputs with noise coefficients that it wasn't trained on? What if we change vary sigma to something it wasn't trained on? This section will explore that</p>
        <p>We vary sigma over a diverse range of values between 0.0-1.0 to get the denoising results of our 5 epoch model.</p>
        <p>Here are some results:</p>
        <div class="image-container">
            <img src="image_design/out_dist_5_1.png" alt="Out Dist 5 epoch">
            <p><em>Out of distribution noised images denoised with 5 epoch of UNet Training</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/out_dist_5_2.png" alt="Out Dist 5 epoch">
            <p><em>Out of distribution noised images denoised with 5 epoch of UNet Training</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/out_dist_5_3.png" alt="Out Dist 5 epoch">
            <p><em>Out of distribution noised images denoised with 5 epoch of UNet Training</em></p>
        </div>
        <p>Pretty good results! We can compare with the results using 1 epoch of UNet Training</p>
        <div class="image-container">
            <img src="image_design/out_dist_1_1.png" alt="Out Dist 1 epoch">
            <p><em>Out of distribution noised images denoised with 1 epoch of UNet Training</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/out_dist_1_2.png" alt="Out Dist 1 epoch">
            <p><em>Out of distribution noised images denoised with 1 epoch of UNet Training</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/out_dist_1_3.png" alt="Out Dist 1 epoch">
            <p><em>Out of distribution noised images denoised with 1 epoch of UNet Training</em></p>
        </div>
        <p>We see that it is clear that the 5 epoch model improved significantly from the 1 epoch model in terms of out of distribution noised test set images. </p>
        <h3>2</h3>
        <p>Now, we train an actual diffusion model. The difference here is that instead of directly predicting the denoised image, we want to predict the noise that was added
            to the clean image during the generation of the noised image. We use this actual diffusion model, so we can mimick the iterative denoising process that we saw in Part A, 
            for a simple dataset like MNIST.
        </p>
        <p>Mathematically, our loss function is now a L2 loss on the noise vector, rather than the resulting (clean) image vector, as follows:</p>
        <div class="equation">
            \[
            L = \mathbb{E}_{\epsilon, x} || \epsilon_{\theta} (z) -\epsilon||^2

            \]
        </div>
        <p>Where epsilon_theta is a UNet denoising model(the diffusion model). </p>
        <p>Our eventual goal is to be able to sample z from pure noise, rather than just simply a noised MNIST image, and have the denoiser model work well anyway.</p>
        <h3>2.1</h3>
        <p>Our training procedure can be described as follows:</p>
        <div class="image-container">
            <img src="image_design/procedure_diffusion_model.png" alt="Diff Model Procedure">
            <p><em>Diffusion model constants and training procedure</em></p>
        </div>
        <p>Our training algorithm can be described as follows:</p>
        <div class="image-container">
            <img src="image_design/algo_tcunet.png" alt="Alg time conditioned unet">
            <p><em>Algorithm for training Time Conditioned UNet</em></p>
        </div>
        <p>We add time conditioning to the UNet, making it a time conditioned UNet. We do this through two pairs of a series of linear layers, which we call the FCBlock. 
            This linear FCBlock takes in a constant (t) and outputs a tensor of shape (D, 1, 1) and (2D, 1, 1) respectively, which are added to the block in the UNet after the 
            first Upblock (before concatenation) and also the Unflatten procedure (before concatenation). This is shown in the below diagram: 
        </p>
        <div class="image-container">
            <img src="image_design/tc_unet_diagram.png" alt="Out Dist 1 epoch">
            <p><em>Time Conditioned UNet Structural Diagram</em></p>
        </div>
        <p>We repeat the training loop for num_epochs=20 epochs, since predicting noise is a more difficult task than denoising. We use a hidden dimension of D=128, and an Adam Optimizer learning rate of 1e-3, with a learning rate scheduler of 
            (0.1)^ (1.0/num_epochs). The scheduler will decrease the learning rate by the factor every epoch. Our batch_size = 128. 
        </p>
        <h3>2.2</h3>
        <p>Our (log) training losses per step graph is as follows:</p>
        <div class="image-container">
            <img src="image_design/tc_unet_loss_graph.png" alt="Log Loss Base">
            <p><em>Log Loss over Steps for Time Conditioned UNet</em></p>
        </div>
        <h3>2.3</h3>
        <p>Now, using our time conditioned model, we can finally sample random noise, and watch as our model (iteratively) transforms the noise into nice images! The algorithm snapshot
            I followed are as follows:
        </p>
        <div class="image-container">
            <img src="image_design/algo2_t_only.png" alt="Tcond algo">
            <p><em>Time Conditional Algorithm Snapshot</em></p>
        </div>
        <p>We compare the results for the model trained with 5 epochs and 20 epochs. Here are our sampling results:</p>
        <div class="image-container">
            <img src="image_design/5_epoch_tcmodel.png" alt="TCond sampling, 5 epoch">
            <p><em>Time Conditional Algorithm Sampling results, 5 epochs</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/20_epoch_tcmodel.png" alt="TCond sampling, 20 epoch">
            <p><em>Time Conditional Algorithm Sampling results, 20 epochs</em></p>
        </div>
        <p>(B&W) Gifs of the interpolation!</p>
        <div class="image-container">
            <img src="image_design/combined_grid.gif" alt="Interpolation Gif, noise to real">
            <p><em>Denoising Gif, 40 images from noise to iteratively denoised image</em></p>
        </div>
        <h3>2.4</h3>
        <p>Now, we would like to condition the output on the class. We would like to be able to force the model to output class-specific images for random noised input, depending
            on a class parameter that we pass in. </p>
        <p>The way we accomplish this is by adding two more FCBlocks, taking in one hot encoded inputs of shape (N, 10), since 10 is the number of classes of this MNIST dataset. 
            The FCBlocks in this case will output, after executing its linear layers, output tensors of shape (N, D) and (2N, D), where D=hidden_dim. These output tensors will affect
            the exact same components of the UNet; the result after the first UpBlock as well as the result after the Unflatten Block. The difference, however, is that the output of the FCBlocks
            here will not be added to the components; it will instead serve as the coefficient of the original blocks, UpBlock and UnFlatten.
        </p>
        <p>We do not want the model to lose the ability to generate images without conditioning, however. My solution is then to pass in a one_hot encoding of all zeroes with a probability of 0.1, forcing the model
            to learn to also be able to generate quality images even when no class conditioning information is provided.
        </p>
        <p>Training is very similar to time conditioned UNet training, with the exact hyperparameters. The only difference being the addition of the class labels of the MNIST dataset to the training.</p>
        <p>Here is the training Algorithm Card:</p>
        <div class="image-container">
            <img src="image_design/algo3_time_class_condition.png" alt="Alg time + class conditioned unet">
            <p><em>Algorithm for training Time and Class Conditioned UNet</em></p>
        </div>
        <p>After training, we get the following (Log) loss vs Steps Graph</p>
        <div class="image-container">
            <img src="image_design/time_classcond_tcmodel.png" alt="time+class condition loss">
            <p><em>Time and Class Conditioned UNet (Log) loss vs Steps</em></p>
        </div>
        <h3>2.5</h3>
        <p>Here our sampling procedure is very similar to the way we did it with time conditioned UNet. However, here we use classifier free guidance to get our final value for noise.

        </p>
        <div class="equation">
            \[
            \epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)
            \]
        </div>
        <p>With gamma equal to 5, and epsilon_u and epsilon_c being the unconditioned (c=0) and conditioned noises</p>
        <p>Like in Part A of the project, we basically extrapolate the noise so that it is further away from the unconditioned noise by 5 times, along the line that connects
            the conditional and unconditional noise vectors. 
        </p>
        <p>Our results for epoch=5 and epoch=20 are as follows:</p>
        <div class="image-container">
            <img src="image_design/time_classcond_5epochs.png" alt="time+class condition sampling 5 epoch">
            <p><em>Time and Class Conditioned Sampling with the 5 epoch model</em></p>
        </div>
        <div class="image-container">
            <img src="image_design/time_classcond_20epochs.png" alt="time+class condition sampling 20 epoch">
            <p><em>Time and Class Conditioned Sampling with the 20 epoch model</em></p>
        </div>
        <p>(B&W) Gifs of the interpolation(Using the best, 20 epoch model)!</p>
        <div class="image-container">
            <img src="image_design/combined_grid_new2.gif" alt="Interpolation Gif, noise to real">
            <p><em>Denoising Gif, 40 images from noise to iteratively denoised image, with class conditioning</em></p>
        </div>
        <h2>Reflection</h2>
        <p>I really enjoyed this project. In fact, it was probably one of my most favourite projects so far! Free from the pain of having to debug too excessively (unlike Project 3 and 4, trying to align images 
            well was a painstakingly time consuming task), this project
        was the most fun project for me and I really really enjoyed the cool results it generated. </p> 
        <p>The project taught me how important iteratively denoising is, and how powerful diffusion models can be when it comes to generating cool images. It also made me realize how 
            difficult a task it is to accurately denoise images. I find the image infilling part to be the most interesting; I did not know that noising the campanile and having it generate high quality images
            could produce such good, but otherwise seemingly random looking images. 
        </p>
    </section>
    <script>
        // Generate Table of Contents
        const toc = document.getElementById("toc");
        const content = document.getElementById("content");
        const headings = content.querySelectorAll("h2, h3");

        headings.forEach((heading) => {
            const link = document.createElement("a");
            link.href = `#${heading.id || heading.textContent.trim().replace(/\s+/g, "-").toLowerCase()}`;
            link.textContent = heading.textContent;
            link.className = `heading-link ${heading.tagName.toLowerCase()}`;
            heading.id = heading.id || heading.textContent.trim().replace(/\s+/g, "-").toLowerCase();
            toc.appendChild(link);
        });
        // Adjust sidebar height dynamically based on header height
        const sidebar = document.getElementById("sidebar");
        const header = document.querySelector("header");

        function adjustSidebarHeight() {
            const headerHeight = header.offsetHeight;
            sidebar.style.height = `calc(100vh - ${headerHeight}px)`;
            sidebar.style.top = `${headerHeight}px`;
        }

        // Run on page load and on window resize
        window.addEventListener("load", adjustSidebarHeight);
        window.addEventListener("resize", adjustSidebarHeight);
        window.addEventListener("scroll", () => {
            if (window.scrollY >= header.offsetHeight) {
                sidebar.style.top = "0";
                sidebar.style.height = "100vh";
            } else {
                adjustSidebarHeight();
            }
        });
    </script>
</body>
</html>
